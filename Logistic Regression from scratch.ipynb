{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this review I do a complete discussion of logistic regression for binary classification. First I perform all the mathematical derivations, including backpropagation and optimisation. Then I discuss a complete Python development of the method without using any available libraries. At the end I apply the method using scikit-learn library and show how to do the same modelling quickly and easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first formulate our problem. Let's suppose we want to be able to classify some observations in two classes. For example we may want to know if there is a cat in an image or not. Or we may want to know if a person will change insurance in the next six months or not.\n",
    "Basically we will have some features that describe what we are observing (for example the RGB values of the pixel of the image, or the age, address, weight of a person) and we want to be able to predict if the observation is of class one or two (cat or no-cat, change insurance or not). This kind of classification is called binary classification.\n",
    "Logistic regression, in its easier form, is used to perform exactly this. \n",
    "\n",
    "Our goal is to be able to have a model that we can train (let it learn from examples) and that can predict if a certain new observation (its input) is of one of two classes.\n",
    "\n",
    "Let's first clarify some notation that we will use in this paper.\n",
    "\n",
    "Our prediction will be a variable $a$ that can only be $0$ or $1$ (we will indicate with $0$ and $1$ the two classes we are trying to predict). In a more mathematical form we will have\n",
    "$$\n",
    "\\text{prediction / estimate}\\ \\ \\rightarrow \\ \\ a\n",
    "\\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "But what our method will give as output, or as a prediction, will be the probability of $a$ being 1, given the input case $x$. Or in a more mathematical form:\n",
    "\n",
    "$$\n",
    "a = P(y = 1 \\ |\\ x)\n",
    "$$\n",
    "\n",
    "usually we will then define an input observation to be of class $1$ if $a>0.5$ and of classe $0$ if $a <= 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have $n_x$ input features (let's assume they are numerical): $x_1, x_2, ..., x_{n_x}$. Those can be written as a vector. We will indicate the vector with $x$ \n",
    "\n",
    "$$\n",
    "x = (x_1, x_2, ..., x_{n_x})\n",
    "$$\n",
    "\n",
    "In our discussion we will also use the vector $w$ that will contain $n_x$ weigths (also numerical) and a constant $b$ (number) (usually called bias):\n",
    "\n",
    "$$\n",
    "w = (w_1, w_2, ..., w_{n_x})\n",
    "$$\n",
    "\n",
    "\n",
    "As you may know, what we need to do is to find the ideal weights $w$ and bias $b$ to classify our observations. \n",
    "\n",
    "Let's suppose for a moment that we have already found the ideal $w$ and $b$.\n",
    "To apply the method we will then have to perform the following steps\n",
    "\n",
    "### Step 1\n",
    "\n",
    "We first build a linear combination $z$ of our inputs using $w$ and $b$\n",
    "\n",
    "$$\n",
    "z = w_1 x_1+w_2 x_2+...+w_{n_x} x_{n_x}+b\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Now it will be very useful to consider our vectors $x$ or $w$ as tensors (or matrices) of dimensions $(n_x, 1)$ (so vertical vectors). This will make the generalisation to many training cases a lot easier, since we will be able to use the formula we will find (and that we will need to vectorize, more on that later) as they are. So we have $x$ and $w$ and both have the dimensions $(n_x,1)$. Equation (1) can then be rewritten as a matrix multiplication (is now simply the inner product of two vectors).\n",
    "\n",
    "$$\n",
    "z = w^T x + b = w_1 x_1+w_2 x_2+...+w_{n_x} x_{n_x}+b\n",
    "$$\n",
    "\n",
    "Note that in Python, as long as we are dealing with tensor of rank 1, it will not make any difference.\n",
    "\n",
    "Consider for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,1])\n",
    "y = np.array([2,2])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this case is easy to see that transposing does not make any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "print(x.T)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(x.T,x))\n",
    "print(np.dot(x,x))\n",
    "print(np.dot(x,x.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we define x as a multidimensional array with ```[[]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "y = np.array([[2],[2]])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have now a $2x1$ tensor, instead of a strange object with dimensions $(2,)$.\n",
    "Now to perform a ```np.dot()``` inner product we will need to use the transpose of the first matrix to be able to obtain a single number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "res = np.dot(x.T,y)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(np.squeeze(res))\n",
    "print(np.squeeze(res).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the difference between the shapes. Remember that to calculate $z$ you will need a float, not a $1x1$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the easiest solution is to simply reshape your arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,1]).reshape(2,1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is probably the easiest solution since you will usually have your input features as a linear vector. So ```reshape()``` will be the easiest solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Since as output we want, as briefly discussion above, only the probabibilty of $a$ being $0$ or $1$, we will use  the sigmoid function applied to $z$\n",
    "\n",
    "$$\n",
    "a = \\sigma (z) = \\sigma (w^T x + b)\n",
    "$$\n",
    "\n",
    "where with $w^T$ we have indicated the transpose of the vector $w$. If before $w$ was an \"horizontal\" vector, or better a matrix with dimension $(1,n_x)$, $w^T$ will be a column vector with dimensions $(n_x,1)$. In this way we can treat $w^T x$ as normal matrix multiplication. The result will be a scalar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational graph for logistic regression algorithm (that will allow us to calculate the derivates of the loss function and the cost function) is the following\n",
    "\n",
    "![title](logistic_regression_computational_graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,1])\n",
    "y = np.array([2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[2 2]\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(x.T,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
