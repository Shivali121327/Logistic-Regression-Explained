{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this review I do a complete discussion of logistic regression for binary classification. First I perform all the mathematical derivations, including backpropagation and optimisation. Then I discuss a complete Python development of the method without using any available libraries. At the end I apply the method using scikit-learn library and show how to do the same modelling quickly and easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's suppose we want to be able to classify some observations in two classes. For example we may want to know if there is a cat in an image or not. Or we may want to know if a person will change insurance in the next six months or not.\n",
    "Basically we will have some features that describe what we are observing (for example the RGB values of the pixel of the image, or the age, address, weight of a person) and we want to be able to predict if the observation is of class one or two (cat or no-cat, change insurance or not). This kind of classification is called binary classification.\n",
    "Logistic regression, in its easier form, is used to perform exactly this. \n",
    "\n",
    "Our goal is to be able to have a model that we can train (let it learn from examples) and that can predict if a certain new observation (its input) is of one of two classes.\n",
    "\n",
    "Let's first clarify some notation that we will use in this paper.\n",
    "\n",
    "Our prediction will be a variable $a$ that can only be $0$ or $1$ (we will indicate with $0$ and $1$ the two classes we are trying to predict). In a more mathematical form we will have\n",
    "$$\n",
    "\\text{prediction / estimate}\\ \\ \\rightarrow \\ \\ a\n",
    "\\in \\{0,1\\}\n",
    "$$\n",
    "\n",
    "But what our method will give as output, or as a prediction, will be the probability of $a$ being 1, given the input case $x$. Or in a more mathematical form:\n",
    "\n",
    "$$\n",
    "a = P(y = 1 \\ |\\ x)\n",
    "$$\n",
    "\n",
    "usually we will then define an input observation to be of class $1$ if $a>0.5$ and of classe $0$ if $a <= 0.5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume we have $n_x$ input features (let's assume they are numerical): $x_1, x_2, ..., x_{n_x}$. Those can be written as a vector. We will indicate the vector with $x$ \n",
    "\n",
    "$$\n",
    "x = (x_1, x_2, ..., x_{n_x})\n",
    "$$\n",
    "\n",
    "In our discussion we will also use the vector $w$ that will contain $n_x$ weigths (also numerical) and a constant $b$ (number) (usually called bias):\n",
    "\n",
    "$$\n",
    "w = (w_1, w_2, ..., w_{n_x})\n",
    "$$\n",
    "\n",
    "\n",
    "As you may know, what we need to do is to find the ideal weights $w$ and bias $b$ to classify our observations. \n",
    "\n",
    "Let's suppose for a moment that we have already found the ideal $w$ and $b$.\n",
    "To apply the method we will then have to perform the following steps\n",
    "\n",
    "### Step 1\n",
    "\n",
    "We first build a linear combination $z$ of our inputs using $w$ and $b$\n",
    "\n",
    "$$\n",
    "z = w_1 x_1+w_2 x_2+...+w_{n_x} x_{n_x}+b\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Now it will be very useful to consider our vectors $x$ or $w$ as tensors (or matrices) of dimensions $(n_x, 1)$ (so vertical vectors). This will make the generalisation to many training cases a lot easier, since we will be able to use the formula we will find (and that we will need to vectorize, more on that later) as they are. So we have $x$ and $w$ and both have the dimensions $(n_x,1)$. Equation (1) can then be rewritten as a matrix multiplication (is now simply the inner product of two vectors).\n",
    "\n",
    "$$\n",
    "z = w^T x + b = w_1 x_1+w_2 x_2+...+w_{n_x} x_{n_x}+b\n",
    "$$\n",
    "\n",
    "Note that in Python, as long as we are dealing with tensor of rank 1, it will not make any difference.\n",
    "\n",
    "Consider for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n",
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.array([1,1])\n",
    "y = np.array([2,2])\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "in this case is easy to see that transposing does not make any difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "print(x.T)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and so we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "print(np.dot(x.T,x))\n",
    "print(np.dot(x,x))\n",
    "print(np.dot(x,x.T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But if we define x as a multidimensional array with ```[[]]```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1],[1]])\n",
    "y = np.array([[2],[2]])\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have now a $2x1$ tensor, instead of a strange object with dimensions $(2,)$.\n",
    "Now to perform a ```np.dot()``` inner product we will need to use the transpose of the first matrix to be able to obtain a single number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4]]\n",
      "(1, 1)\n"
     ]
    }
   ],
   "source": [
    "res = np.dot(x.T,y)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "print(np.squeeze(res))\n",
    "print(np.squeeze(res).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the difference between the shapes. Remember that to calculate $z$ you will need a float, not a $1x1$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually the easiest solution is to simply reshape your arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [1]]\n",
      "(2, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,1]).reshape(2,1)\n",
    "print(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is probably the easiest solution since you will usually have your input features as a linear vector. So ```reshape()``` will be the quicker way to get yout data in the right shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2\n",
    "\n",
    "Since as output we want, as briefly discussion above, only the probabibilty of $a$ being $0$ or $1$, we will use  the sigmoid function applied to $z$\n",
    "\n",
    "$$\n",
    "a = \\sigma (z) = \\sigma (w^T x + b)\n",
    "$$\n",
    "\n",
    "where with $w^T$ we have indicated the transpose of the vector $w$ (as already explained above). \n",
    "\n",
    "Now we have assumed to have already found the ideal weights $w$ and bias $b$ so the $a$ we get from the previous equation is already our prediction. We don't need to do anything else. But the whole idea of Machine Learning is, well, learning. So that means that we need to find the ideal $w$ and $b$ given a set of features (the $x$ in our notation).\n",
    "But how to learn? What we will do is we will need to minimize the error we get from our prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "\n",
    "To find the best parameters $w$ and $b$, we will need to minimize what is called the cost function (CF). A complete discussion about it is outside the scope of this paper. The CF we will use is the cross entropy (to be precise the following equation is for the Loss function, but as will be clear later on, for one observation the Loss and the Cost function will be the same).\n",
    "\n",
    "$$\n",
    "\\mathscr{L} (a,y) = -\\left[ y \\log a + (1-y) \\log (1-a) \\right]\n",
    "$$\n",
    "\n",
    "It is useful to draw a computational graph to show the steps necessary to calculate $\\mathscr{L} (a,y)$.\n",
    "Please note that we are still discussing a single training observation. We will generalize the approach in the next section.\n",
    "\n",
    "So to reiterate the idea we will need to find the parameters $w$ and $b$ that minimize $\\mathscr{L} (a,y)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computational graph for logistic regression algorithm  is the following\n",
    "\n",
    "![title](logistic_regression_computational_graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To minimize $\\mathscr{L} (a,y)$ we will use the gradient descent algorithm. For a theoretical discussion of the method (well worth knowing) you can refer to several books (or to get an idea from Wikipedia: https://en.wikipedia.org/wiki/Gradient_descent)\n",
    "\n",
    "So to be able to use the algorithm we will need to have all the partial derivatives of $\\mathscr{L} (a,y)$ with respect to the weights $w_j$ (with $j \\in {1,..., n_x})$ and $b$. So we will need to calculate\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j}\n",
    "\\tag{2}\n",
    "$$\n",
    "and\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial b}\n",
    "\\tag{3}\n",
    "$$\n",
    "\n",
    "then during the \"descent\", at step $n+1$ we will need to update the parameters at step $n$ with the formulas\n",
    "\n",
    "$$\\displaystyle\n",
    "w_{j, n+1} = w_{j,n} -\\gamma \\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j}\n",
    "\\tag{3a}\n",
    "$$\n",
    "and\n",
    "$$\\displaystyle\n",
    "b_{n+1} = b_{n} -\\gamma \\frac{\\partial \\mathscr{L} (a,y)}{\\partial b}\n",
    "\\tag{3b}\n",
    "$$\n",
    "\n",
    "where $\\gamma$ is called the learning rate and is an additional parameter that needs to be optimized. But for the moment we will consider it as a constant.\n",
    "\n",
    "So if we want to implement the entire algorithm from scratch we will need explicit equations for the partial derivatives. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To calculate the derivatives in equation (2) and (3) we can simply apply the chain rule (for those who knows calculus). $\\mathscr{L} (a,y)$ is simply the composition of several function. So we can easily write (again try to derive it yourself)\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial w_j}\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial b} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial b}\n",
    "\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can easily calculate all the derivates appearing in (4) and (5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} = - \\frac{y}{a}+ \\frac{1-y}{1-a}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{da}{dz} = a(1-a)\n",
    "$$\n",
    "\n",
    "and finally\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial z}{\\partial w_j} = x_j\n",
    "$$\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial z}{\\partial b} = 1\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we put together those equations with (4) and (5) we get\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial w_j} = (a-y) \\ x_j\n",
    "\\tag{4}\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial \\mathscr{L} (a,y)}{\\partial b} = \\frac{\\partial \\mathscr{L} (a,y)}{\\partial a} \\frac{da}{dz}\\frac{\\partial z}{\\partial b} = (a-y)\n",
    "\\tag{5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that (3a) and (3b) can be written easily as\n",
    "\n",
    "$$\\displaystyle\n",
    "w_{j, n+1} = w_{j,n} -\\gamma \\frac{\\partial \\mathscr{L} (a,y)}{\\partial w_j} = w_{j,n} - \\gamma \\ (a-y) \\ x_j\n",
    "\\tag{6}\n",
    "$$\n",
    "and\n",
    "$$\\displaystyle\n",
    "b_{n+1} = b_{n} -\\gamma \\frac{\\partial \\mathscr{L} (a,y)}{\\partial b} = b_n - \\gamma \\ (a-y)\n",
    "\\tag{7}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation (6) and (7) will be the ones we will use in our gradient descent implementation. If you have heard about backpropagation and computational graphs the following graph will get you immediately equations (4) and (5) (at least the first part, to obtain the $(a-y)x_j$ and $(a-y)$ you still need to perform some derivatives). Remember that in backpropagation you go from right to left, instead that from left to right. Each unit is divided in two halfs. The right one is used in the forward propagation and the left one in the backpropagation. This method is widely used in neural networks, and especially in deep learning. Since logistic regression can be obtained by a neural network with a single sigmoid unit we can use this method to get the right equations for the gradient descent.\n",
    "\n",
    "![Computational Graph for the backpropagation algorithm](logistic_regression_computational_graph2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalisation to many training cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to generalise our equation to the case when we have many training cases. This will be easier than expected. First some notation. Let's indicate with $x^{(i)}$ our $i$th training case. Remember $x^{(i)}$ have the dimension $(n_x,1)$. So they are basically \"vertical\" vectors. Or better formulated $x^{(i)}$ is a $\\mathbb{R}^{n_x \\times 1}$ matrix. Let's indicate with $X$ our matrix containing all the features and all the training cases. So $X$ will be a matrix of dimensions $\\mathbb{R}^{n_x \\times m}$ where with $m$ we have indicated the number of training cases that we have. $X$ can be obtained stacking vertically all the $x^{(i)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So let's rewrite our equations in matrix form:\n",
    "\n",
    "$$\n",
    "Z = w^T X + B = (\n",
    "\\begin{matrix}\n",
    "w^T x^{(1)}+b & w^T x^{(2)}+b & ... & w^T x^{(m)}+b  \\\\\n",
    "\\end{matrix}\n",
    ")\n",
    "$$\n",
    "\n",
    "where we have indicated with $B$ a matrix of dimension $(1,n_x)$ with all elements equal to $b$. In Python we will not need to define this additional vector since **broadcasting** will take care of it.\n",
    "Note that $Z$ will have the dimensions $(1, m)$, since $w^T$ has dimensions $(1,n_x)$ and $X$ has $(n_x,m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will need to calculate $A$. This will be a matrix of dimensions $(1,m)$ that will be\n",
    "\n",
    "$$\n",
    "A = \\sigma (Z) = [\n",
    "\\begin{matrix}\n",
    "\\sigma (w^T x^{(1)}+b) &\\sigma (w^T x^{(2)}+b) & ... & \\sigma (w^T x^{(m)}+b)  \\\\\n",
    "\\end{matrix}\n",
    "]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I mentioned before that the Loss function and the cost function for one training case are the same. When we are dealing with many training cases we will define our cost function as the average of our loss function calculated over all training cases as:\n",
    "\n",
    "$$\\displaystyle\n",
    "J(w,b) = \\frac{1}{m} \\sum_{i=1}^{m} \\mathscr{L} (a^{(i)},y^{(i)})\n",
    "$$\n",
    "\n",
    "Now we will need to calculate the derivative of our new cost function to implement the backpropagation algorithm to determine our parameters. This will not be difficult, since the derivative of the sum of functions is the sum of derivatives so we have simply\n",
    "\n",
    "$$\\displaystyle\n",
    "\\frac{\\partial J(w,b)}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m} \n",
    "\\frac{\\partial J(w,b)}{\\partial a^{(i)}} \\frac{da^{(i)}}{dz^{(i)}}\\frac{\\partial z^{(i)}}{\\partial w_j} = \\frac{1}{m} \\sum_{i=1}^{m}(a^{(i)}-y^{(i)}) \\ x^{(i)}_j\n",
    "\\tag{8}\n",
    "$$\n",
    "\n",
    "Now we can define the following matrices\n",
    "$$\\displaystyle\n",
    "A = \\left[\n",
    "\\begin{matrix}\n",
    "a^{(1)}& a^{(2)} & ... & a^{(m)}  \\\\\n",
    "\\end{matrix}\n",
    "\\right] \\ \\ \\ \\in \\mathbb{R}^{1 \\times m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
